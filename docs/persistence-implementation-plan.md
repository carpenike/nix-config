ğŸ§© Homelab Storage, Backup, and Service Architecture (NixOS + ZFS + S3 + PostgreSQL)

Goal: Build a declarative, self-healing, and testable homelab environment using NixOS, ZFS, and S3-based backups â€” inspired by CloudNativePG but adapted for a single-node, break-friendly setup where experimentation is encouraged.

â¸»

ğŸ Quick Start (Phase 0 MVP)
    1.  Create ZFS pool tank with datasets under /persist
    2.  Configure Sanoid for hourly/daily snapshots
    3.  Set up Restic to back up /persist â†’ S3/B2
    4.  Test manual restore (restic restore â†’ temporary path)
    5.  Done â€” you now have a working 3-2-1 baseline

      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     snapshots     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   ZFS Pool   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Backup     â”‚
      â”‚   tank/â€¦     â”‚                   â”‚   Server     â”‚
      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                                  â”‚
           â–¼                                  â–¼
         Restic                           S3 / B2


â¸»

âš™ï¸ Core Principles

Principle   Description
Declarative-first   All datasets + services defined in Nix; reproducible rebuilds
Service-isolated    Each service (Plex, Sonarr, Radarr, Home Assistant, PostgreSQL) gets its own dataset
Self-healing    preseed@.service restores from ZFS â†’ Restic â†’ S3 if data missing
Disaster-resilient  Sanoid + Syncoid + Restic + Barman provide full 3-2-1 coverage
Testable    Restores and PITR validated regularly (break it â†’ restore it)


â¸»

ğŸ—‚ï¸ Service Data Layout

tank/persist/
â”œâ”€â”€ plex
â”œâ”€â”€ sonarr
â”œâ”€â”€ radarr
â”œâ”€â”€ home-assistant
â””â”€â”€ pg
    â”œâ”€â”€ data
    â””â”€â”€ wal

Typical properties:

Dataset recordsize  compression notes
plex    128K    lz4 media caches
sonarr/radarr   16K lz4 small SQLite
home-assistant  16K lz4 frequent writes
pg/data 8K  lz4 Postgres pages
pg/wal  128K    off sequential WAL I/O


â¸»

ğŸª„ Pre-Seeding & Restore Logic

Behavior

If a serviceâ€™s dataset or directory is missing:
    1.  Attempt ZFS receive from backup pool
    2.  If unavailable â†’ Restic restore from S3
    3.  If still unavailable â†’ skip + log warning

Systemd Ordering

zfs-mount.service
   â†“
preseed@sonarr.service (TimeoutStartSec=3600, Before=sonarr.service)
   â†“
sonarr.service

All services After=preseed@%i.service to ensure restores finish first.

Logging

Each preseed logs:
    â€¢ Data source (ZFS/Restic)
    â€¢ Duration
    â€¢ Checksum verification
    â€¢ â€œSkipped restoreâ€ if data already present

â¸»

ğŸ› ï¸ Backup & Replication

Local
    â€¢ Hourly + daily Sanoid snapshots
    â€¢ Syncoid â€“no-destroy to secondary pool (prevents backup deletion)
    â€¢ Periodic ZFS scrubs + alerts

Offsite
    â€¢ Restic to S3 / B2 (encrypted by provider)
    â€¢ Lifecycle: 30 days hot â†’ archive â†’ delete @ 90 days
    â€¢ Typical cost: ~$6/mo @ B2 for 100 GB

Decision Matrix

Need    Use Notes
Fast local rollback ZFS snapshot    sub-second
Replica copy    Syncoid no destroy flag
Offsite dedup backup    Restic â†’ S3   encrypted
Database PITR   Barman-Cloud    CNPG-style


â¸»

ğŸ—„ï¸ PostgreSQL (CNPG-Inspired)

Architecture
    â€¢ One Postgres cluster (pg_main) initially
    â€¢ Barman-Cloud handles WAL archival + base backups

archive_mode = on
archive_command = 'barman-cloud-wal-archive s3://pg-backups %p'

âš ï¸ PITR Limitations
    â€¢ All-or-nothing restore (instance-wide)
    â€¢ Recovery = base backup + WAL replay (may take hours)
    â€¢ Requires complete WAL sequence
    â€¢ Use separate clusters only for isolation or version differences

â¸»

ğŸ” Failure Scenarios & Mitigations

Failure Mitigation
S3 unavailable  Restore from ZFS
Both ZFS + S3 unavailable   Manual import or re-seed from base images
Preseed failure Timeout + retry (3x) â†’ service skip + alert
Syncoid lag > 24 h  Alert â†’ force manual send
WAL archive gap Prometheus alert + base backup restart
Bit rot in backup   Checksums validated via Restic verify


â¸»

ğŸ§ª Restore Testing Checklist
    â€¢ ZFS snapshot rollback per-service
    â€¢ ZFS send/receive from backup pool
    â€¢ Restic restore (full + incremental)
    â€¢ PostgreSQL PITR (1 h / 1 d / 7 d)
    â€¢ Preseed automation on fresh host
    â€¢ Cross-host restore (A â†’ B)
    â€¢ Partial restore (single service)

Track restore time, success rate, and manual steps to measure complexity.

â¸»

ğŸ“Š Monitoring Targets

zfs:
  - pool_capacity (>80%)
  - scrub_errors
  - fragmentation
  - last_snapshot_age
syncoid:
  - last_successful_run
  - lag_seconds
restic:
  - last_backup_age
  - verify_errors
barman:
  - wal_archive_lag
  - last_base_backup
postgres:
  - wal_rate_mb_per_hour
  - archive_command_failures

Expose via Prometheus â†’ Grafana dashboard.

â¸»

ğŸ” Security & Secrets
    â€¢ ZFS encryption: disabled (homelab)
    â€¢ S3: SSE-S3 provider-side encryption
    â€¢ Transport: SSH for Syncoid, HTTPS for S3
    â€¢ Secrets: managed via sops-nix
    â€¢ Rotation: manual on key change

â¸»

ğŸ§© Nix Implementation Pattern

# modules/storage/zfs-dataset.nix
{ lib, config, ... }:
let
  mkDataset = { pool, name, mount, props ? {} }: {
    systemd.services."zfs-create-${name}" = {
      description = "Ensure ZFS dataset ${pool}/${name}";
      wantedBy = [ "local-fs.target" ];
      serviceConfig.ExecStart = ''
        /run/current-system/sw/bin/zfs create -p ${pool}/${name} || true
      '';
    };
    fileSystems.${mount}.device = "${pool}/${name}";
    fileSystems.${mount}.fsType = "zfs";
  };
in {
  config = mkDataset { pool = "tank"; name = "persist/sonarr"; mount = "/var/lib/sonarr"; };
}

# /etc/barman.d/homelab-pg.conf
[homelab-pg]
backup_method = postgres
archiver = on
streaming_archiver = on
slot_name = barman
create_slot = auto
backup_compression = gzip


â¸»

ğŸ“‰ Complexity & Rollback
    â€¢ MVP (Phase 0): ZFS + Restic only
    â€¢ Rollback Plan: if automation breaks, disable preseed and rely on snapshot restores
    â€¢ Measure Complexity: can you restore in < 1 hour? if not, simplify

â¸»

ğŸ§­ Monitoring & Failure Response
    â€¢ Alert if:
    â€¢ ZFS pool > 80%
    â€¢ Syncoid lag > 24 h
    â€¢ WAL archive lag > 1 h
    â€¢ Restic snapshot age > 24 h
    â€¢ Actions: automatic retries, manual runbook updates

â¸»

ğŸ§° Runbook & Troubleshooting (Skeleton)

Disaster Recovery Runbook
    1.  Verify hardware + bootable system
    2.  Import ZFS pool â†’ check datasets
    3.  If missing, restic restore latest â†’ /persist/<svc>
    4.  Run systemctl start preseed@all
    5.  Confirm services healthy

Troubleshooting
    â€¢ syncoid errors â†’ re-run manually with --debug
    â€¢ restic check â†’ verify repo integrity
    â€¢ barman-cloud-check â†’ test S3 connectivity

â¸»

ğŸ“– Glossary

Term    Meaning
PITR    Point-in-Time Recovery
WAL Write-Ahead Log (Postgres)
Syncoid ZFS replication wrapper
Sanoid  ZFS snapshot scheduler
Restic  Encrypted dedup backup tool
Barman-Cloud    PostgreSQL S3 PITR utility


â¸»

ğŸ’¡ Final Notes

âœ… All-in design â€” declarative, modular, testable
âœ… Can break safely â€” restore paths proven
âœ… CNPG-grade PITR â€” without Kubernetes
âœ… ZFS resilience + S3 offsite = self-healing lab

â€œIf you canâ€™t restore it, you donâ€™t own it.â€
This architecture ensures you always can.
