---
version: "3"

tasks:
  orchestrate:
    desc: Trigger all backup systems before deployment
    silent: true
    vars:
      host: '{{.host | default "forge"}}'
      NIXOS_DOMAIN: '{{.NIXOS_DOMAIN | default "holthome.net"}}'
    summary: |
      Triggers all backup systems (Sanoid, Syncoid, Restic, pgBackRest) on the
      specified host before deployment. This is a safety measure to ensure you
      have recent backups before making major system changes.

      Args:
        host: Target host (default: forge)
        NIXOS_DOMAIN: Domain suffix (default: holthome.net)

      What it does:
        - Stage 0: Pre-flight checks (disk space validation)
        - Stage 1: Creates ZFS snapshots (Sanoid)
        - Stage 2: Replicates to NAS (Syncoid - parallel)
        - Stage 3a: PostgreSQL full backup (pgBackRest - sequential)
        - Stage 3b: Application backups (Restic - limited parallel)
        - Stage 4: Verification and reporting

      Expected duration: 30-90 minutes

      Exit codes:
        0 = All backups completed successfully
        1 = Partial failure (<50%) - acceptable for deployment
        2 = Critical failure (>50%) - DO NOT deploy

      Examples:
        task backup:orchestrate
        task backup:orchestrate host=forge
    cmds:
      - ssh {{.host}}.{{.NIXOS_DOMAIN}} "sudo backup-orchestrator"
    preconditions:
      - sh: which ssh
        msg: "ssh not found"
      - sh: ssh {{.host}}.{{.NIXOS_DOMAIN}} "which backup-orchestrator" 2>/dev/null
        msg: |
          backup-orchestrator not installed on {{.host}}.{{.NIXOS_DOMAIN}}

          Please deploy the package first:
            task nix:apply-nixos host={{.host}}

  orchestrate-dry-run:
    desc: Preview backup orchestration without executing
    silent: true
    vars:
      host: '{{.host | default "forge"}}'
      NIXOS_DOMAIN: '{{.NIXOS_DOMAIN | default "holthome.net"}}'
    summary: |
      Dry-run mode shows what would be executed without actually running any backups.
      Useful for validating service discovery and checking which backups would trigger.

      Args:
        host: Target host (default: forge)
    cmds:
      - ssh {{.host}}.{{.NIXOS_DOMAIN}} "sudo backup-orchestrator --dry-run"
    preconditions:
      - sh: which ssh
        msg: "ssh not found"
      - sh: ssh {{.host}}.{{.NIXOS_DOMAIN}} "which backup-orchestrator" 2>/dev/null
        msg: "backup-orchestrator not installed on {{.host}}.{{.NIXOS_DOMAIN}}"

  orchestrate-verbose:
    desc: Trigger backups with detailed progress output
    silent: true
    vars:
      host: '{{.host | default "forge"}}'
      NIXOS_DOMAIN: '{{.NIXOS_DOMAIN | default "holthome.net"}}'
    summary: |
      Verbose mode provides detailed progress information including:
        - Individual service status updates
        - Timing information
        - Debug logging

      Useful for troubleshooting or monitoring long-running backups.

      Args:
        host: Target host (default: forge)
    cmds:
      - ssh {{.host}}.{{.NIXOS_DOMAIN}} "sudo backup-orchestrator --verbose"
    preconditions:
      - sh: which ssh
        msg: "ssh not found"
      - sh: ssh {{.host}}.{{.NIXOS_DOMAIN}} "which backup-orchestrator" 2>/dev/null
        msg: "backup-orchestrator not installed on {{.host}}.{{.NIXOS_DOMAIN}}"

  orchestrate-automated:
    desc: Trigger backups without prompts (for automation/CI)
    silent: true
    vars:
      host: '{{.host | default "forge"}}'
      NIXOS_DOMAIN: '{{.NIXOS_DOMAIN | default "holthome.net"}}'
    summary: |
      Automated mode skips all confirmation prompts and runs backups immediately.
      Returns JSON output for parsing by monitoring systems.

      Intended for:
        - CI/CD pipelines
        - Automated deployment workflows
        - Monitoring/alerting integrations

      Args:
        host: Target host (default: forge)
    cmds:
      - ssh {{.host}}.{{.NIXOS_DOMAIN}} "sudo backup-orchestrator --yes --json"
    preconditions:
      - sh: which ssh
        msg: "ssh not found"
      - sh: ssh {{.host}}.{{.NIXOS_DOMAIN}} "which backup-orchestrator" 2>/dev/null
        msg: "backup-orchestrator not installed on {{.host}}.{{.NIXOS_DOMAIN}}"

  status:
    desc: Show current status of all backup systems
    silent: false
    vars:
      host: '{{.host | default "forge"}}'
      NIXOS_DOMAIN: '{{.NIXOS_DOMAIN | default "holthome.net"}}'
      PROMETHEUS_URL: 'https://prom.{{.NIXOS_DOMAIN}}'
    summary: |
      Displays a consolidated dashboard of all backup system statuses by querying
      Prometheus metrics. Shows:
        - pgBackRest: Last backup time, repo status, backup type
        - Syncoid: ZFS replication status to nas-1
        - Restic: Application backups to both NAS and B2

      The dashboard highlights:
        - OK: Green (backup completed < 26 hours ago)
        - STALE: Yellow (backup > 26 hours old)
        - FAILED: Red (last run failed)
        - RUNNING: Cyan (currently in progress)

      Args:
        host: Target host with Prometheus (default: forge)

      Environment Variables (required):
        PROMETHEUS_API_KEY: API key for Prometheus authentication (X-Api-Key header)

      Examples:
        task backup:status
        task backup:status host=forge
    cmds:
      - backup-status --prometheus-url {{.PROMETHEUS_URL}}
    preconditions:
      - sh: which backup-status
        msg: "backup-status command not found. Has it been installed via home-manager?"
      - sh: test -n "$PROMETHEUS_API_KEY"
        msg: "PROMETHEUS_API_KEY environment variable not set"

  pgbackrest-info:
    desc: Show pgBackRest backup info for both repositories
    silent: true
    vars:
      host: '{{.host | default "forge"}}'
      NIXOS_DOMAIN: '{{.NIXOS_DOMAIN | default "holthome.net"}}'
      FORMATTER: '{{.ROOT_DIR}}/scripts/format-pgbackrest.py'
    summary: |
      Displays pgBackRest backup information for both repositories:
        - Repo1 (NFS): /mnt/nas-postgresql/pgbackrest
        - Repo2 (R2): s3://nix-homelab-prod-servers/forge-pgbackrest

      Args:
        host: Target host (default: forge)

      Examples:
        task backup:pgbackrest-info
        task backup:pgbackrest-info host=forge
    cmds:
      - |
        ssh {{.host}}.{{.NIXOS_DOMAIN}} "sudo -u postgres bash -c '
          source /run/secrets/restic/r2-prod-env
          export PGBACKREST_REPO2_S3_KEY=\$AWS_ACCESS_KEY_ID
          export PGBACKREST_REPO2_S3_KEY_SECRET=\$AWS_SECRET_ACCESS_KEY
          pgbackrest --stanza=main \\
            --repo2-type=s3 \\
            --repo2-path=/forge-pgbackrest \\
            --repo2-s3-bucket=nix-homelab-prod-servers \\
            --repo2-s3-endpoint=21ee32956d11b5baf662d186bd0b4ab4.r2.cloudflarestorage.com \\
            --repo2-s3-region=auto \\
            --repo2-s3-uri-style=path \\
            --output=json \\
            info
        '" 2>&1 | python3 {{.FORMATTER}}
    preconditions:
      - sh: which ssh
        msg: "ssh not found"
      - sh: which python3
        msg: "python3 not found"

  list:
    desc: List all backups (Restic + pgBackRest)
    silent: false
    vars:
      host: '{{.host | default "forge"}}'
      NIXOS_DOMAIN: '{{.NIXOS_DOMAIN | default "holthome.net"}}'
      PROMETHEUS_URL: 'https://prom.{{.NIXOS_DOMAIN}}'
      FORMATTER: '{{.ROOT_DIR}}/scripts/format-pgbackrest.py'
    summary: |
      Lists backup status for all backup systems:
        - Restic: Application backups (via Prometheus metrics)
        - pgBackRest: PostgreSQL backups (via SSH)

      This is a combined view of both backup:restic and backup:pgbackrest.

      Args:
        host: Target host (default: forge)

      Examples:
        task backup:list
        task backup:list host=forge
    cmds:
      - |
        echo ""
        echo "=============================="
        echo "  Restic Backup Status"
        echo "=============================="
        echo ""
        backup-list --host {{.host}} --domain {{.NIXOS_DOMAIN}} --prometheus-url {{.PROMETHEUS_URL}}
        echo ""
        ssh {{.host}}.{{.NIXOS_DOMAIN}} "sudo -u postgres bash -c '
          source /run/secrets/restic/r2-prod-env
          export PGBACKREST_REPO2_S3_KEY=\$AWS_ACCESS_KEY_ID
          export PGBACKREST_REPO2_S3_KEY_SECRET=\$AWS_SECRET_ACCESS_KEY
          pgbackrest --stanza=main \\
            --repo2-type=s3 \\
            --repo2-path=/forge-pgbackrest \\
            --repo2-s3-bucket=nix-homelab-prod-servers \\
            --repo2-s3-endpoint=21ee32956d11b5baf662d186bd0b4ab4.r2.cloudflarestorage.com \\
            --repo2-s3-region=auto \\
            --repo2-s3-uri-style=path \\
            --output=json \\
            info
        '" 2>&1 | python3 {{.FORMATTER}}
    preconditions:
      - sh: which backup-list
        msg: "backup-list command not found. Has it been installed via home-manager?"
      - sh: test -n "$PROMETHEUS_API_KEY"
        msg: "PROMETHEUS_API_KEY environment variable not set"
      - sh: which ssh
        msg: "ssh not found"

  restic:
    desc: List Restic backup status from Prometheus metrics (fast)
    silent: false
    vars:
      host: '{{.host | default "forge"}}'
      NIXOS_DOMAIN: '{{.NIXOS_DOMAIN | default "holthome.net"}}'
      PROMETHEUS_URL: 'https://prom.{{.NIXOS_DOMAIN}}'
    summary: |
      Lists Restic backup status for all services by querying Prometheus metrics.
      This is the fast default mode - uses cached metrics from last backup run.

      Shows for each backup job:
        - Status (success/failure)
        - Last backup time
        - Duration
        - Snapshot count
        - Size added
        - Repository health

      Args:
        host: Target host (default: forge)

      Additional arguments can be passed after --:
        --service NAME  Filter by service name (partial match)
        --repo NAME     Filter by repository name
        --verify        Query restic directly (slower, validates data)
        --json          JSON output for scripting

      Environment Variables (required for Prometheus mode):
        PROMETHEUS_API_KEY: API key for Prometheus authentication

      Examples:
        task backup:restic
        task backup:restic host=forge
        task backup:restic -- --service sonarr
        task backup:restic -- --repo nas-primary
    cmds:
      - backup-list --host {{.host}} --domain {{.NIXOS_DOMAIN}} --prometheus-url {{.PROMETHEUS_URL}} {{.CLI_ARGS}}
    preconditions:
      - sh: which backup-list
        msg: "backup-list command not found. Has it been installed via home-manager?"
      - sh: test -n "$PROMETHEUS_API_KEY"
        msg: "PROMETHEUS_API_KEY environment variable not set. Use 'task backup:restic-verify' for direct restic queries."

  restic-verify:
    desc: Verify Restic backups by querying repositories directly (slow)
    silent: false
    vars:
      host: '{{.host | default "forge"}}'
      NIXOS_DOMAIN: '{{.NIXOS_DOMAIN | default "holthome.net"}}'
    summary: |
      Verifies backup data by directly querying each restic repository via SSH.
      This is slower but validates that snapshots are actually accessible.

      Use this when:
        - Prometheus metrics seem stale or incorrect
        - You need to verify repository integrity
        - You want to see actual snapshot details (IDs, times, tags)

      Args:
        host: Target host (default: forge)

      Additional arguments can be passed after --:
        --service NAME  Filter by service name (partial match)
        --repo NAME     Filter by repository name
        --limit N       Maximum snapshots per service (default: 10)
        --json          JSON output for scripting

      Examples:
        task backup:restic-verify
        task backup:restic-verify -- --service sonarr
        task backup:restic-verify -- --limit 5
    cmds:
      - backup-list --host {{.host}} --domain {{.NIXOS_DOMAIN}} --verify {{.CLI_ARGS}}
    preconditions:
      - sh: which backup-list
        msg: "backup-list command not found. Has it been installed via home-manager?"
      - sh: which ssh
        msg: "ssh not found"

  restic-json:
    desc: List Restic backup status in JSON format (for scripting)
    silent: true
    vars:
      host: '{{.host | default "forge"}}'
      NIXOS_DOMAIN: '{{.NIXOS_DOMAIN | default "holthome.net"}}'
      PROMETHEUS_URL: 'https://prom.{{.NIXOS_DOMAIN}}'
    summary: |
      Lists Restic backup status in JSON format for scripting and automation.
      Uses Prometheus metrics by default (add --verify for direct queries).

      Args:
        host: Target host (default: forge)

      Environment Variables (required for Prometheus mode):
        PROMETHEUS_API_KEY: API key for Prometheus authentication

      Output Format (Prometheus mode):
        {
          "host": "forge.holthome.net",
          "mode": "prometheus",
          "services": [
            {
              "job_name": "service-name",
              "repository_name": "nas-primary",
              "status": 1,
              "last_success": 1732806000,
              "snapshots": 42,
              "healthy": true
            }
          ],
          "summary": {
            "total_services": N,
            "healthy_services": N,
            "total_snapshots": N
          }
        }

      Examples:
        task backup:restic-json
        task backup:restic-json | jq '.services[].job_name'
        task backup:restic-json | jq '.summary.healthy_services'
        task backup:restic-json -- --verify  # Direct restic queries (no API key needed)
    cmds:
      - backup-list --host {{.host}} --domain {{.NIXOS_DOMAIN}} --prometheus-url {{.PROMETHEUS_URL}} --json {{.CLI_ARGS}}
    preconditions:
      - sh: which backup-list
        msg: "backup-list command not found. Has it been installed via home-manager?"
      - sh: test -n "$PROMETHEUS_API_KEY"
        msg: "PROMETHEUS_API_KEY environment variable not set"

  pgbackrest:
    desc: Show pgBackRest backup status (PostgreSQL)
    silent: true
    vars:
      host: '{{.host | default "forge"}}'
      NIXOS_DOMAIN: '{{.NIXOS_DOMAIN | default "holthome.net"}}'
    summary: |
      Displays pgBackRest backup status for PostgreSQL databases.

      Shows a formatted summary including:
        - Repository status (NFS and R2)
        - Backup counts by type (full/incr/diff)
        - Latest backup timestamps with age
        - Recent incremental backups
        - WAL archive range
        - Storage summary

      Args:
        host: Target host (default: forge)

      Examples:
        task backup:pgbackrest
        task backup:pgbackrest host=forge
    cmds:
      - |
        ssh {{.host}}.{{.NIXOS_DOMAIN}} "sudo -u postgres bash -c '
          source /run/secrets/restic/r2-prod-env
          export PGBACKREST_REPO2_S3_KEY=\$AWS_ACCESS_KEY_ID
          export PGBACKREST_REPO2_S3_KEY_SECRET=\$AWS_SECRET_ACCESS_KEY
          pgbackrest --stanza=main \
            --repo2-type=s3 \
            --repo2-path=/forge-pgbackrest \
            --repo2-s3-bucket=nix-homelab-prod-servers \
            --repo2-s3-endpoint=21ee32956d11b5baf662d186bd0b4ab4.r2.cloudflarestorage.com \
            --repo2-s3-region=auto \
            --repo2-s3-uri-style=path \
            --output=json \
            info
        '" 2>/dev/null | python3 {{.ROOT_DIR}}/scripts/format-pgbackrest.py
    preconditions:
      - sh: which ssh
        msg: "ssh not found"
      - sh: which python3
        msg: "python3 not found"

  pgbackrest-metrics-check:
    desc: Check pgBackRest metrics collection status
    silent: true
    vars:
      host: '{{.host | default "forge"}}'
      NIXOS_DOMAIN: '{{.NIXOS_DOMAIN | default "holthome.net"}}'
    summary: |
      Checks the status of pgBackRest metrics collection:
        - Service status
        - Last run time
        - Metrics file content

      Args:
        host: Target host (default: forge)

      Examples:
        task backup:pgbackrest-metrics-check
    cmds:
      - |
        echo "=== pgBackRest Metrics Service Status ==="
        ssh {{.host}}.{{.NIXOS_DOMAIN}} "systemctl status pgbackrest-metrics.service --no-pager"
        echo ""
        echo "=== Last Metrics Collection ==="
        ssh {{.host}}.{{.NIXOS_DOMAIN}} "sudo tail -30 /var/lib/node_exporter/textfile_collector/pgbackrest.prom"
    preconditions:
      - sh: which ssh
        msg: "ssh not found"

  # ============================================================================
  # Syncoid ZFS Replication Tasks
  # ============================================================================

  syncoid:
    desc: List Syncoid ZFS replication status from Prometheus metrics (fast)
    silent: false
    vars:
      host: '{{.host | default "forge"}}'
      NIXOS_DOMAIN: '{{.NIXOS_DOMAIN | default "holthome.net"}}'
      PROMETHEUS_URL: 'https://prom.{{.NIXOS_DOMAIN}}'
    summary: |
      Lists Syncoid ZFS replication status for all configured datasets by querying
      Prometheus metrics. This is the fast default mode - uses cached metrics.

      Shows for each replication job:
        - Dataset name
        - Target host/location
        - Status (OK/STALE/FAILED/RUNNING)
        - Last successful replication time

      Args:
        host: Target host (default: forge)

      Additional arguments can be passed after --:
        --dataset NAME   Filter by dataset name (partial match)
        --target NAME    Filter by target host (partial match)
        --verify         Query systemd directly (slower, validates data)
        --json           JSON output for scripting

      Environment Variables (required for Prometheus mode):
        PROMETHEUS_API_KEY: API key for Prometheus authentication

      Examples:
        task backup:syncoid
        task backup:syncoid host=forge
        task backup:syncoid -- --dataset sonarr
        task backup:syncoid -- --target nas-1
    cmds:
      - syncoid-list --host {{.host}} --domain {{.NIXOS_DOMAIN}} --prometheus-url {{.PROMETHEUS_URL}} {{.CLI_ARGS}}
    preconditions:
      - sh: which syncoid-list
        msg: "syncoid-list command not found. Has it been installed via home-manager?"
      - sh: test -n "$PROMETHEUS_API_KEY"
        msg: "PROMETHEUS_API_KEY environment variable not set. Use 'task backup:syncoid-verify' for direct queries."

  syncoid-verify:
    desc: Verify Syncoid replication by querying systemd directly (slow)
    silent: false
    vars:
      host: '{{.host | default "forge"}}'
      NIXOS_DOMAIN: '{{.NIXOS_DOMAIN | default "holthome.net"}}'
    summary: |
      Verifies replication status by directly querying systemd units via SSH.
      This is slower but validates that services are actually running/succeeded.

      Use this when:
        - Prometheus metrics seem stale or incorrect
        - You need to verify actual systemd unit status
        - You want to see systemd state details

      Args:
        host: Target host (default: forge)

      Additional arguments can be passed after --:
        --dataset NAME   Filter by dataset name (partial match)
        --target NAME    Filter by target host (partial match)
        --json           JSON output for scripting

      Examples:
        task backup:syncoid-verify
        task backup:syncoid-verify -- --dataset sonarr
    cmds:
      - syncoid-list --host {{.host}} --domain {{.NIXOS_DOMAIN}} --verify {{.CLI_ARGS}}
    preconditions:
      - sh: which syncoid-list
        msg: "syncoid-list command not found. Has it been installed via home-manager?"
      - sh: which ssh
        msg: "ssh not found"

  syncoid-json:
    desc: List Syncoid replication status in JSON format (for scripting)
    silent: true
    vars:
      host: '{{.host | default "forge"}}'
      NIXOS_DOMAIN: '{{.NIXOS_DOMAIN | default "holthome.net"}}'
      PROMETHEUS_URL: 'https://prom.{{.NIXOS_DOMAIN}}'
    summary: |
      Lists Syncoid replication status in JSON format for scripting and automation.
      Uses Prometheus metrics by default (add --verify for direct queries).

      Args:
        host: Target host (default: forge)

      Environment Variables (required for Prometheus mode):
        PROMETHEUS_API_KEY: API key for Prometheus authentication

      Output Format:
        {
          "host": "forge.holthome.net",
          "mode": "prometheus",
          "stale_threshold_hours": 2,
          "jobs": [
            {
              "dataset": "tank/services/sonarr",
              "unit": "syncoid-tank-services-sonarr",
              "target_host": "nas-1.holthome.net",
              "target_name": "NFS",
              "target_location": "nas-1",
              "status": 1,
              "status_text": "OK",
              "last_success": 1732806000,
              "details": "Healthy"
            }
          ],
          "summary": {
            "total": N,
            "ok": N,
            "running": N,
            "stale": N,
            "failed": N,
            "healthy": N
          }
        }

      Examples:
        task backup:syncoid-json
        task backup:syncoid-json | jq '.jobs[].dataset'
        task backup:syncoid-json | jq '.summary.healthy'
        task backup:syncoid-json -- --verify  # Direct systemd queries (no API key needed)
    cmds:
      - syncoid-list --host {{.host}} --domain {{.NIXOS_DOMAIN}} --prometheus-url {{.PROMETHEUS_URL}} --json {{.CLI_ARGS}}
    preconditions:
      - sh: which syncoid-list
        msg: "syncoid-list command not found. Has it been installed via home-manager?"
      - sh: test -n "$PROMETHEUS_API_KEY"
        msg: "PROMETHEUS_API_KEY environment variable not set"

  syncoid-trigger:
    desc: Manually trigger all Syncoid replication jobs
    silent: false
    vars:
      host: '{{.host | default "forge"}}'
      NIXOS_DOMAIN: '{{.NIXOS_DOMAIN | default "holthome.net"}}'
    summary: |
      Manually triggers all Syncoid replication jobs on the target host.
      Useful for forcing immediate replication before a deployment or maintenance.

      Args:
        host: Target host (default: forge)

      Examples:
        task backup:syncoid-trigger
        task backup:syncoid-trigger host=forge
    cmds:
      - |
        echo "Triggering all Syncoid replication jobs on {{.host}}.{{.NIXOS_DOMAIN}}..."
        ssh {{.host}}.{{.NIXOS_DOMAIN}} "sudo systemctl start syncoid.target"
        echo "Syncoid jobs triggered. Use 'task backup:syncoid' to monitor status."
    preconditions:
      - sh: which ssh
        msg: "ssh not found"

  syncoid-logs:
    desc: Show recent Syncoid replication logs
    silent: false
    vars:
      host: '{{.host | default "forge"}}'
      NIXOS_DOMAIN: '{{.NIXOS_DOMAIN | default "holthome.net"}}'
      dataset: '{{.dataset | default ""}}'
    summary: |
      Shows recent logs from Syncoid replication services.

      Args:
        host: Target host (default: forge)
        dataset: Optional dataset filter (e.g., "sonarr" to see syncoid-*-sonarr logs)

      Examples:
        task backup:syncoid-logs
        task backup:syncoid-logs dataset=sonarr
        task backup:syncoid-logs host=forge dataset=plex
    cmds:
      - |
        {{if .dataset}}
        echo "=== Syncoid logs for *{{.dataset}}* on {{.host}}.{{.NIXOS_DOMAIN}} ==="
        ssh {{.host}}.{{.NIXOS_DOMAIN}} "sudo journalctl -u 'syncoid-*{{.dataset}}*' --since '2 hours ago' --no-pager"
        {{else}}
        echo "=== All Syncoid logs on {{.host}}.{{.NIXOS_DOMAIN}} (last 2 hours) ==="
        ssh {{.host}}.{{.NIXOS_DOMAIN}} "sudo journalctl -u 'syncoid-*' --since '2 hours ago' --no-pager | tail -100"
        {{end}}
    preconditions:
      - sh: which ssh
        msg: "ssh not found"

  syncoid-metrics-check:
    desc: Check Syncoid metrics collection status
    silent: false
    vars:
      host: '{{.host | default "forge"}}'
      NIXOS_DOMAIN: '{{.NIXOS_DOMAIN | default "holthome.net"}}'
    summary: |
      Checks the status of Syncoid metrics collection:
        - Lists all syncoid metric files
        - Shows content of metrics

      Args:
        host: Target host (default: forge)

      Examples:
        task backup:syncoid-metrics-check
    cmds:
      - |
        echo "=== Syncoid Metrics Files on {{.host}}.{{.NIXOS_DOMAIN}} ==="
        ssh {{.host}}.{{.NIXOS_DOMAIN}} "ls -la /var/lib/node_exporter/textfile_collector/syncoid_*.prom 2>/dev/null || echo 'No syncoid metrics files found'"
        echo ""
        echo "=== Syncoid Metrics Content ==="
        ssh {{.host}}.{{.NIXOS_DOMAIN}} "cat /var/lib/node_exporter/textfile_collector/syncoid_*.prom 2>/dev/null | grep -v '^#' | sort || echo 'No metrics available'"
    preconditions:
      - sh: which ssh
        msg: "ssh not found"
